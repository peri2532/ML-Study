# 벡터의 개념과 연산

## ML 관점에서 벡터란?

- 머신러닝에서 하나의 데이터는 보통 하나의 벡터로 표현된다.
    - 예: 키, 몸무게, 나이 → (170, 65, 25)
    - 예: 문서 → TF-IDF 벡터
    - 예: 이미지 → 픽셀값 벡터

- 즉, 벡터는 "하나의 객체를 수치적으로 표현한 것"이다.

## 벡터의 개념

- 스칼라 
    - 크기만으로 표현되는 양
    - 하나의 실수값으로 나타낸다.
    - r,s,t 같은 소문자 이탤릭체로 표현한다.

- 벡터
    - 크기와 방향을 함께 가지는 양
    - 여러 성분의 순서쌍(또는 좌표)으로 나타낸다.
    - $\mathbf{u}, \mathbf{v}, \mathbf{w}$ 같은 굵은 글씨체의 소문자로 표현한다.

## 위치벡터와 데이터 해석

- 위치벡터는 원점으로부터의 위치를 나타낸다.
- ML에서는 원점이 "기준 상태"를 의미하기도 한다.
    - 예: 평균을 뺀 데이터(centering)
    - 원점은 평균 상태

- 따라서 데이터 분석에서는
  "평균을 원점으로 이동시키는 것"이 매우 중요하다.
  (PCA에서 반드시 수행)

## 벡터의 크기와 손실함수

- 벡터의 크기: ||u|| = sqrt(u·u)
- 오차 벡터의 크기는 예측이 얼마나 틀렸는지를 의미한다.

- 회귀에서:
  ||y - ŷ||^2 는 총 오차 크기이며,
  이는 MSE(평균제곱오차)의 핵심 구조이다.
- \( \mathbb{R}^2 \)에서  
\[
\mathbf{u} = (a,b)
\]
의 크기는

\[
\|\mathbf{u}\| = \sqrt{a^2 + b^2}
\]

이다.

일반적으로 \( \mathbb{R}^n \)에서  
\[
\mathbf{u} = (a_1, a_2, \dots, a_n)
\]
이면

\[
\|\mathbf{u}\| = \sqrt{a_1^2 + a_2^2 + \cdots + a_n^2}
\]

이다.


- 영벡터
    - 모든 성분이 0인 벡터
    \[
\mathbf{0} = (0,0,\dots,0)
\]

## 단위벡터의 의미

- 단위벡터는 "순수한 방향 정보"만 남긴 벡터이다.
- ML에서 정규화(normalization)는
  벡터의 크기 영향을 제거하고 방향만 비교하기 위해 사용된다.

- 코사인 유사도는 단위벡터를 이용한 비교라고 볼 수 있다.
 

- 단위좌표벡터
    \( \mathbb{R}^2 \)에서

\[
\mathbf{e}_1 = (1,0), \quad \mathbf{e}_2 = (0,1)
\]

임의의 벡터 \( (a,b) \) 는

\[
(a,b) = a\mathbf{e}_1 + b\mathbf{e}_2
\]

로 나타낼 수 있다.

## 기저벡터와 특징 공간

- 표준기저 e1, e2, ... 는 각 축을 의미한다.
- 각 축은 하나의 feature(특징)라고 해석할 수 있다.

- 예:
  (a, b, c) = a e1 + b e2 + c e3
  → 데이터는 각 feature의 가중합으로 표현된다.

- PCA는 새로운 "기저벡터"를 찾는 과정이다.

# 문제 1
- \( \mathbb{R}^2 \)의 부분집합

\( 
W = \left\{ \mathbf{x} \in \mathbb{R}^2 \;\middle|\; 
\mathbf{x} = 
\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix}, \; x_1 + x_2 = 2 
\right\}
\) 을 기하학적으로 해석하시오.

# 문제 1 정답

\( x_1 = x, \; x_2 = y \) 로 두면

\[
x + y = 2
\]

즉,

\[
y = -x + 2
\]이므로 기울기 -1, y절편 2인 직선이다.

# 깨달음 점
내가 헷갈린점: x^2 + y^2 = r^2인줄 알고 원으로 그렸음
똑바로 문제를 읽을것

## 벡터의 연산

u = (u1, ..., un)
v = (v1, ..., vn)
c는 실수(스칼라)

## 벡터 덧셈과 스칼라곱의 ML 의미
### 1) 덧셈
- 벡터 덧셈: 데이터 결합 또는 평균 계산
- 스칼라곱: feature의 중요도(weight) 조절

- 선형모델:
  y = w1 x1 + w2 x2 + ... + wn xn
  → 스칼라곱의 확장 형태
u + v = (u1 + v1, ..., un + vn)

### 2) 뺄셈
u - v = (u1 - v1, ..., un - vn)

### 3) 스칼라곱

c u = (c u1, ..., c un)

---

## 벡터 연산의 성질

u, v, w는 R^n의 벡터
α, β는 스칼라(실수)

### 교환법칙
u + v = v + u

### 덧셈의 결합법칙
(u + v) + w = u + (v + w)

### 덧셈의 항등원
u + 0 = u

### 덧셈의 역원
u + (-u) = 0

### 스칼라의 분배법칙
α(u + v) = αu + αv

### 스칼라 합에 대한 분배법칙
(α + β)u = αu + βu

### 스칼라 결합법칙
α(βu) = (αβ)u

### 곱셈의 항등원
1u = u

### 영벡터 성질
0u = 0

# 문제 2

n차원 벡터와 (n-1)차원 벡터 사이의 연산이 가능한지 판단하시오.

## 정답

일반적으로 불가능하다.
## ML 관점

- 차원이 다르면 비교 자체가 불가능하다.
- 따라서 모든 데이터는 같은 feature 공간에 있어야 한다.
- 이것이 feature engineering이 중요한 이유이다.
## 이유:

\( \mathbb{R}^n \) 과 \( \mathbb{R}^{n-1} \) 는 서로 다른 벡터공간이므로  
성분의 개수가 다르다.

따라서

- 덧셈, 뺄셈: 각 성분끼리 연산할 수 없으므로 정의되지 않는다.
- 내적: 성분 개수가 달라 정의되지 않는다.
- 스칼라곱: 스칼라와 벡터 사이의 연산이므로 두 벡터 사이의 연산이 아니다.

즉, 같은 벡터공간에 속한 벡터들끼리만 연산이 가능하다.

(단, 선형변환을 통해 같은 공간으로 보내면 가능하다.)

---

# 문제 3

화학방정식을 벡터로 표현하시오.

### (1) 2NH_2 + H_2 -> 2NH_3

각 분자를 (N, H) 좌표로 나타내면

\[
NH_2 = (1,2), \quad
H_2 = (0,2), \quad
NH_3 = (1,3)
\]

따라서

\[
2(1,2) + (0,2) = 2(1,3)
\]

---

### (2) CO + H_2O -> H_2 + CO_2

각 분자를 (C, H, O) 좌표로 나타내면

\[
CO = (1,0,1)
\]

\[
H_2O = (0,2,1)
\]

\[
H_2 = (0,2,0)
\]

\[
CO_2 = (1,0,2)
\]

따라서


(1,0,1) + (0,2,1)= (0,2,0) + (1,0,2)
## 선형결합의 의미

- 화학방정식은 벡터의 선형결합 문제이다.
- 이는 계수(스칼라)를 조절하여 균형을 맞추는 문제이다.
- 머신러닝의 선형모델도 동일하게
  feature들의 선형결합으로 출력이 만들어진다.